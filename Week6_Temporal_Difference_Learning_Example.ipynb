{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Temporal Difference Learning Copy of Week1B.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPNdV0vpL7IKnYXQ/i22Rvy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcnica89/MATH4060/blob/main/Week6_Temporal_Difference_Learning_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UnMnJvcFDlmG"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "from jax import random as jrandom\n",
        "from jax import nn as jnn\n",
        "from jax import jit\n",
        "import random\n",
        "import time\n",
        "import timeit\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "plt.rc('xtick', labelsize=15) \n",
        "plt.rc('ytick', labelsize=15)\n",
        "font = {'size'   : 20}\n",
        "\n",
        "plt.rc('font', **font)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gambler's Ruin (aka Drunkard's Walk)\n",
        "\n",
        "Consider the statespace $[[0,N_{target}]] \\subset \\mathbb{Z}$ which represetnt possible values for a gambler's wealth. Let $X_n$ be a Markov chain representing the Gambler's wealth after $n$ bets. If they have no money, $0$, then they cannot make any bets:\n",
        "\n",
        "$$P( X_{n+1} = 0 | X_n = 0) =1$$\n",
        "\n",
        "If they reach their target $N_{target}$, then they choose to stop gambling.\n",
        "\n",
        "\n",
        "$$P( X_{n+1} = N_{target} | X_n = N_{target}) =1$$\n",
        "\n",
        "Otherwise, they bet and their fortune goes up or down by exactly $1$:\n",
        "\n",
        "$$P( X_{n+1} = x+1 | X_n = x) =p$$\n",
        "$$P( X_{n+1} = x-1 | X_n = x) =1-p$$\n",
        "\n",
        "The gambler keeps playing until they either go broke or reach their target. Assume the Gambler gains a reward of 1 life point if they get to $N_{target}$ and the gain a reward of 0 if they reach their target. Determine\n",
        "\n",
        "$$E[\\text{Reward}|X_0 = x] = P(\\text{Reach target}|X_0 = x)$$\n"
      ],
      "metadata": {
        "id": "N46NjQCKD4AC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Monte Carlo Method"
      ],
      "metadata": {
        "id": "qJ3cfgYWtDTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def monte_carlo_method(learning_rate, N_episodes, N_target):\n",
        "  value_function = np.zeros(N_target+1)\n",
        "  num_visits = np.zeros(N_target+1)\n",
        "\n",
        "  for i in range(N_episodes):\n",
        "    x_0 = random.randint(1,N_target-1)\n",
        "    current_state = x_0\n",
        "\n",
        "    current_time = 0\n",
        "    max_time = 1000\n",
        "    state_history = np.zeros(max_time+1,dtype=int)\n",
        "    reward_history = np.zeros(max_time+1)\n",
        "\n",
        "    while True: \n",
        "      state_history[current_time] = current_state\n",
        "      if current_state==0 or current_state==N_target or current_time >= max_time:\n",
        "        reward_history[current_time] = (current_state==N_target)\n",
        "        break\n",
        "      else:\n",
        "        reward_history[current_time] = 0   \n",
        "      \n",
        "      #THIS IS THE MOST IMPORTANT LINE OF PART I:\n",
        "      #current_state = gridworld_next_state(current_state,policy[current_state])\n",
        "      \n",
        "      current_state += 2*random.randint(0,1)-1\n",
        "      current_time += 1\n",
        "\n",
        "    t_final = current_time\n",
        "    \n",
        "    \n",
        "    #PART II\n",
        "    #Now update the value function for states we saw this episode\n",
        "    #We are using the first visit monte carlo rule!\n",
        "    \n",
        "    visited_already = np.zeros(16,dtype=bool)\n",
        "    for t in range(0,t_final+1):\n",
        "      state = state_history[t]\n",
        "      if visited_already[state] == False:\n",
        "        G = np.sum(reward_history[t:t_final+1])\n",
        "        num_visits[state] += 1\n",
        "        \n",
        "        #This line actually computes the AVERAGE of G over all the visits!\n",
        "        #Homework: Figure out why this line is actually computing the average!\n",
        "        #i..e value_function[state] will be = average of G's over all observations\n",
        "        \n",
        "        value_function[state] += learning_rate*(G - value_function[state])\n",
        "\n",
        "        visited_already[state] = True\n",
        "  \n",
        "  return value_function"
      ],
      "metadata": {
        "id": "zWRnWNlStZWu"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "monte_carlo_method(0.05,100000, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVTM2tkez9eD",
        "outputId": "8262a627-964d-4dfc-9c10-191c2d55e8cf"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.12626279, 0.18175374, 0.23327819, 0.32603477,\n",
              "       0.53419425, 0.60018212, 0.75812825, 0.78688499, 0.89392174,\n",
              "       1.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TD0 Method (Temperal Difference Learning with 0 memory)"
      ],
      "metadata": {
        "id": "Swga9qT8s88k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def TD0_method(learning_rate, N_samples, N_target):\n",
        "\n",
        "  #This code is very similar to the naive Monte Carlo method\n",
        "  #Instead of keeping track of total_rewards, we update the function V as we go\n",
        "\n",
        "  value_func = np.zeros(N_target+1)\n",
        "  value_func[N_target] = 1\n",
        "  \n",
        "  for i in range(N_samples):\n",
        "    x_0 = random.randint(1,N_target-1)\n",
        "    X = x_0\n",
        "    \n",
        "    while X > 0 and X < N_target:\n",
        "      new_X = X + 2*random.randint(0,1)-1\n",
        "      delta_value = value_func[new_X] - value_func[X]\n",
        "      value_func[X] += learning_rate*( delta_value  )\n",
        "      X = new_X\n",
        "\n",
        "  return value_func"
      ],
      "metadata": {
        "id": "br_rg8wGv-xk"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TD0_method(0.1, 100000, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtuWr6mZ1heA",
        "outputId": "1c3bccc6-179c-4439-a6eb-46974bc07f91"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.1209138 , 0.21034023, 0.28481609, 0.35772368,\n",
              "       0.45067095, 0.51319773, 0.69971957, 0.80599985, 0.89852383,\n",
              "       1.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Time comparisons"
      ],
      "metadata": {
        "id": "Sa7nl51f1xhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%time monte_carlo_method(0.01, 100000, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2HREgFv1w5k",
        "outputId": "565a12bb-0edc-4009-a925-a5daa5dc2220"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 14.2 s, sys: 1.1 s, total: 15.3 s\n",
            "Wall time: 14.1 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.07245991, 0.1765066 , 0.2687439 , 0.3971212 ,\n",
              "       0.49495066, 0.60143898, 0.68979404, 0.78990505, 0.8927356 ,\n",
              "       1.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time TD0_method(0.01, 300000, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_IYpof917vb",
        "outputId": "e843c257-497a-469e-91c6-dd80fed246c7"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 12.2 s, sys: 1.48 ms, total: 12.2 s\n",
            "Wall time: 12.2 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.08949829, 0.20449964, 0.2966078 , 0.40519875,\n",
              "       0.51030072, 0.62275848, 0.7215919 , 0.81953907, 0.90139822,\n",
              "       1.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    }
  ]
}